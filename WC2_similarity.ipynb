{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_list = [\n",
    "    \n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_1_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_2_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_3_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_4_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_5_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_6_7.csv',\n",
    "    '/Users/rainfow/Desktop/Summer projects/companies_data/datetime_7_7.csv'\n",
    "]\n",
    "\n",
    "# 从CSV文件读取数据，并将日期列转换为datetime类型\n",
    "df_list = [pd.read_csv(f, parse_dates=[\"IncorporationDate\"]) for f in file_list]\n",
    "\n",
    "# 合并所有数据框为一个大的数据框\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.dropna(subset=['lat', 'long'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_postcode_prefix(df, prefix):\n",
    "    # Split the postcode at the space to get the prefix\n",
    "    df['PostCodePrefix'] = df['RegAddress.PostCode'].str.split().str[0]\n",
    "    # Filter the DataFrame based on the prefix\n",
    "    df_filtered = df[df['PostCodePrefix'].str.startswith(prefix)]\n",
    "    return df_filtered\n",
    "\n",
    "df_filtered = filter_by_postcode_prefix(df, 'WC2')\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#df_filtered['Date'] = df_filtered['IncorporationDate'].dt.to_period('D')\n",
    "df_filtered['Date'] = pd.to_datetime(df_filtered['IncorporationDate'])\n",
    "\n",
    "\n",
    "##df_filtered_count = df_filtered.groupby(['RegAddress.PostCode', 'Date']).size().reset_index(name='CompanyCount')\n",
    "##print(df_filtered_count.head(50))\n",
    "\n",
    "df_filtered_count = df_filtered.groupby(['RegAddress.PostCode', 'Date']).agg({\n",
    "    'CompanyName': lambda x: list(x),\n",
    "    'lat': 'first',\n",
    "    'long': 'first',\n",
    "    'SICCode.SicText_1': lambda x: list(x),\n",
    "}).reset_index()\n",
    "\n",
    "# df_filtered_count['CompanyCount'] = df_filtered.groupby(['RegAddress.PostCode', 'Date']).size().values\n",
    "\n",
    "df_filtered_count.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check_CompanyName(df):\n",
    "    # Define the scoring function for company names\n",
    "    def score_company_name(company_name_list):\n",
    "        for company_name in company_name_list:\n",
    "            # Check conditions\n",
    "            # 1. Starts with a digit\n",
    "            #if re.match(r'^\\d', company_name):\n",
    "            #   return 1\n",
    "            # 2. Starts with consecutive digits\n",
    "            if re.match(r'^\\d{3,}', company_name):\n",
    "                return 1.5\n",
    "            # 3. Starts with a special character (!, @, #, etc.)\n",
    "            if re.match(r'^[!@#\\$%\\^&\\*\\(\\)_\\+\\-=\\[\\]\\{\\};:\\'\",<>\\?]', company_name):\n",
    "                return 3\n",
    "            # 4. Starts with three or more consecutive identical letters\n",
    "            if re.match(r'^(.)\\1{2,}', company_name):\n",
    "                return 3\n",
    "            # 5.Name length is less than 8 and starts with a number followed by a letter\n",
    "            if len(company_name) < 8 or re.match(r'^\\d+[A-Za-z]', company_name):\n",
    "                return 3\n",
    "        return 0\n",
    "    \n",
    "    # Calculate score for each row\n",
    "    df['name_score'] = df['CompanyName'].apply(score_company_name)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def integrated_similarity_scores(df):\n",
    "    \n",
    "    # 1. First function: compute_similarity_scores\n",
    "    def compute_string_similarity(companies_1, companies_2):\n",
    "        combined_companies = companies_1 + companies_2\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(combined_companies)\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "        total_similarity = 0\n",
    "        count = 0\n",
    "        for i in range(len(companies_1)):\n",
    "            for j in range(len(companies_1), len(combined_companies)):\n",
    "                total_similarity += cosine_similarities[i][j]\n",
    "                count += 1\n",
    "        return total_similarity / count if count != 0 else 0\n",
    "\n",
    "    def compute_name_score(group):\n",
    "        scores = [0] * len(group)\n",
    "        if len(group) >= 2:\n",
    "            # 找出间隔最短的两个日期的索引\n",
    "            min_diff = float('inf')\n",
    "            idx1, idx2 = None, None\n",
    "            sorted_dates = sorted(range(len(group)), key=lambda x: group.iloc[x]['Date'])\n",
    "            \n",
    "            for i in range(len(sorted_dates) - 1):\n",
    "                date1 = pd.to_datetime(group.iloc[sorted_dates[i]]['Date'])\n",
    "                date2 = pd.to_datetime(group.iloc[sorted_dates[i + 1]]['Date'])\n",
    "                date_diff = (date2 - date1).days\n",
    "                if date_diff < min_diff:\n",
    "                    min_diff = date_diff\n",
    "                    idx1, idx2 = sorted_dates[i], sorted_dates[i + 1]\n",
    "\n",
    "            # 获取间隔最短的两个日期的公司名称列表\n",
    "            recent_companies_1 = group.iloc[idx1]['CompanyName']\n",
    "            recent_companies_2 = group.iloc[idx2]['CompanyName']\n",
    "            if isinstance(recent_companies_1, str):\n",
    "                recent_companies_1 = [recent_companies_1]\n",
    "            if isinstance(recent_companies_2, str):\n",
    "                recent_companies_2 = [recent_companies_2]\n",
    "            # 计算公司名称的平均相似性\n",
    "            avg_similarity = compute_string_similarity(recent_companies_1, recent_companies_2)\n",
    "            threshold = 0.5\n",
    "            if avg_similarity > threshold:\n",
    "                scores[idx1] = 1\n",
    "                scores[idx2] = 1\n",
    "        return scores\n",
    "    \n",
    "    # 2. Second function: compute_siccode_similarity_scores\n",
    "    def compute_siccode_score(group):\n",
    "        scores = [0] * len(group)\n",
    "        if len(group) >= 2:\n",
    "            name_scores = compute_name_score(group)\n",
    "            for i in range(len(name_scores)):\n",
    "                \n",
    "                # Only consider rows with similarity score of 1\n",
    "                if name_scores[i] == 1:  \n",
    "                    if i + 1 < len(name_scores) and name_scores[i + 1] == 1:\n",
    "                        if group.iloc[i]['SICCode.SicText_1'] == group.iloc[i + 1]['SICCode.SicText_1']:\n",
    "                            scores[i] = 1\n",
    "                            scores[i + 1] = 1\n",
    "        return scores\n",
    "    \n",
    "    # Applying functions and adding new columns to df\n",
    "    df['name_similarity_score'] = [score for sublist in df.groupby('RegAddress.PostCode').apply(compute_name_score).reset_index(drop=True) for score in sublist]\n",
    "    df['siccode_similarity_score'] = [score for sublist in df.groupby('RegAddress.PostCode').apply(compute_siccode_score).reset_index(drop=True) for score in sublist]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both scoring methods and then aggregate for the final result\n",
    "def compute_combined_scores(df):\n",
    "    # Get the scores from both methods\n",
    "    df_name_scores = check_CompanyName(df)\n",
    "    df_similarity_scores = integrated_similarity_scores(df)\n",
    "    \n",
    "    # Combine both scores\n",
    "    df['name_score'] = df_name_scores['name_score']\n",
    "    df['name_similarity_score'] = df_similarity_scores['name_similarity_score']\n",
    "    df['siccode_similarity_score'] = df_similarity_scores['siccode_similarity_score']\n",
    "\n",
    "    \n",
    "    # Calculate the total score\n",
    "    df['Total_Score'] = df['name_score'] + df['name_similarity_score']+df['siccode_similarity_score']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Compute the combined scores\n",
    "df_text_scores = compute_combined_scores(df_filtered_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by postcode to get the total score for each postcode\n",
    "similarity_scores_output = df_text_scores.groupby(['RegAddress.PostCode', 'lat', 'long']).agg({\n",
    "    'CompanyName': 'size',\n",
    "    'Total_Score': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "similarity_scores_output.rename(columns={'CompanyName': 'CompanyCount'}, inplace=True)\n",
    "similarity_scores_output.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Basic statistical description of Total_Score\n",
    "score_description = similarity_scores_output[\"Total_Score\"].describe()\n",
    "\n",
    "# Plot the histogram for Total_Score\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(similarity_scores_output[\"Total_Score\"], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Total Scores\")\n",
    "plt.xlabel(\"Total Score\")\n",
    "plt.ylabel(\"Number of Postcodes\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "score_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Define the color and size mapping based on the quantiles of 'SumScores'\n",
    "def map_color_size(score):\n",
    "    threshold_1 = 0\n",
    "    threshold_2 = 1\n",
    "    threshold_3 = 2\n",
    "\n",
    "    if score <= threshold_1:\n",
    "        return 'green', 5, 'Not very suspicious'\n",
    "    elif score <= threshold_2:\n",
    "        return 'blue', 10, 'Slightly suspicious'\n",
    "    elif score <= threshold_3:\n",
    "        return 'orange', 20, 'Fairly suspicious'\n",
    "    else:\n",
    "        return 'red', 100, 'Very suspicious'\n",
    "\n",
    "similarity_scores_output['color'], similarity_scores_output['size'], similarity_scores_output['label'] = zip(*similarity_scores_output['Total_Score'].apply(map_color_size))\n",
    "\n",
    "# Calculate the total number of postcodes\n",
    "total_postcodes = len(similarity_scores_output)\n",
    "\n",
    "# Define a dictionary for labels, colors, and sizes\n",
    "label_dict = {\n",
    "    'Not very suspicious': {'color': 'green', 'size': 5},\n",
    "    'Slightly suspicious': {'color': 'blue', 'size': 10},\n",
    "    'Fairly suspicious': {'color': 'orange', 'size': 20},\n",
    "    'Very suspicious': {'color': 'red', 'size': 100}\n",
    "}\n",
    "\n",
    "# Scatter plot\n",
    "for label, attr in label_dict.items():\n",
    "    idx = similarity_scores_output['label'] == label\n",
    "    postcode_count = len(similarity_scores_output.loc[idx])\n",
    "    percentage = postcode_count / total_postcodes * 100\n",
    "    ax.scatter(similarity_scores_output.loc[idx, 'long'], similarity_scores_output.loc[idx, 'lat'], c=attr['color'], s=attr['size'], label=f'{label} ({percentage:.1f}%)')\n",
    "\n",
    "\n",
    "    # If the label is \"Very suspicious\" (which corresponds to red), add the postal code text\n",
    "    if label == \"Very suspicious\":\n",
    "        for x, y, postcode in zip(similarity_scores_output.loc[idx, 'long'], similarity_scores_output.loc[idx, 'lat'], similarity_scores_output.loc[idx, 'RegAddress.PostCode']):\n",
    "            ax.text(x, y, postcode, ha='right', fontsize=7.5, color='black')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title=\"Suspicion Level\")\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, crs='EPSG:4326', source=ctx.providers.Stamen.TonerLite)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0dc6da6a0e964db3fc43c0c4b9cdc57237008d3e2a908ee4190f21dcea209cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
